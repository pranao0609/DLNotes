# ğŸ§® 8. Gradient Descent and Optimization Algorithms

---

## ğŸ“˜ What is Gradient Descent?

**Gradient Descent** is an optimization algorithm used to minimize the **loss function** by updating the modelâ€™s parameters (weights and biases) in the direction of the **negative gradient**.

---

## ğŸ” Gradient Descent Formula

```

Î¸ = Î¸ - Î± \* âˆ‡L(Î¸)

````

Where:
- `Î¸` = model parameters (weights)
- `Î±` = learning rate (step size)
- `âˆ‡L(Î¸)` = gradient of loss w.r.t. parameters

---

## ğŸš¦ Types of Gradient Descent

| Type              | Description |
|-------------------|-------------|
| **Batch GD**      | Uses the entire dataset to compute gradients (slow but stable) |
| **Stochastic GD** | Uses one sample at a time (fast but noisy) |
| **Mini-batch GD** | Uses a small batch of samples (best of both worlds) |

---

## âš™ï¸ Optimization Algorithms

### 1. **Stochastic Gradient Descent (SGD)**

- Updates weights using one data point at a time  
- Highly fluctuating but often finds good minima

```python
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
````

---

### 2. **Momentum**

* Accelerates SGD by adding a fraction of the previous update
* Helps escape local minima and smooths oscillations

```python
optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
```

---

### 3. **AdaGrad**

* Adapts learning rate individually for each parameter
* Good for sparse data
* Downside: learning rate shrinks too much

---

### 4. **RMSProp**

* Similar to AdaGrad, but uses exponential moving average of squared gradients
* Handles non-stationary objectives well

```python
optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001)
```

---

### 5. **Adam (Adaptive Moment Estimation)**

* Combines Momentum and RMSProp
* Maintains moving average of gradients and squared gradients
* Widely used and generally performs well

```python
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
```

---

## ğŸ“Š Comparison Table

| Optimizer | Adaptive LR | Momentum | Common Use         |
| --------- | ----------- | -------- | ------------------ |
| SGD       | âŒ No        | Optional | Simple tasks       |
| Momentum  | âŒ No        | âœ… Yes    | Faster convergence |
| AdaGrad   | âœ… Yes       | âŒ No     | Sparse data        |
| RMSProp   | âœ… Yes       | âŒ No     | RNNs, NLP          |
| Adam      | âœ… Yes       | âœ… Yes    | Most DL tasks      |

---

## ğŸ§  Learning Rate Tuning

* Too high â†’ overshooting, unstable
* Too low â†’ slow convergence
* Can use **schedulers** to adjust dynamically

```python
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)
```

---

## âœ… Key Points

* Gradient descent is core to learning in deep networks.
* Optimizers affect speed, stability, and final performance.
* Adam is the go-to optimizer in most cases.
* Choosing the right optimizer and learning rate is critical.

---
