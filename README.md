# üß† DL Notes

A structured collection of Deep Learning theory and key concepts, organized topic-wise. This repository is meant to help you learn, revise, and reference fundamental deep learning concepts, math, and examples ‚Äî focused purely on core DL.

---

## üìö Table of Contents

### üî∞ Fundamentals
- Introduction to Deep Learning
- Artificial Neural Networks (ANN)
- Perceptron and Multi-Layer Perceptron (MLP)
- Forward and Backward Propagation
- Activation Functions
- Loss Functions
- Cost Functions vs Loss Functions
- Gradient Descent and Its Variants
- Epochs, Batches, and Iterations
- Weight Initialization Techniques

---

### ‚öôÔ∏è Optimization & Training
- Optimization Algorithms (SGD, Momentum, Adam, etc.)
- Learning Rate and Schedulers
- Regularization Techniques (L1, L2, Dropout)
- Early Stopping
- Batch Normalization
- Layer Normalization
- Overfitting vs Underfitting
- Bias-Variance Tradeoff
- Hyperparameter Tuning

---

### üß† Neural Network Architectures
- Deep Feedforward Networks
- Convolutional Neural Networks (CNNs)
- Pooling Layers (Max Pooling, Average Pooling)
- Recurrent Neural Networks (RNNs)
- Long Short-Term Memory (LSTM)
- Gated Recurrent Unit (GRU)
- Residual Networks (ResNet)
- Autoencoders

---

